{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hWbxFdPJ8Uv4"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import random\n",
        "import json\n",
        "from nltk.corpus import wordnet\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import LinearSVC\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KFPQ59NG8YdI"
      },
      "outputs": [],
      "source": [
        "intents = {\n",
        "  \"intents\": [\n",
        "    {\n",
        "      \"tag\": \"greeting\",\n",
        "      \"patterns\": [\"Hi\", \"Hello\", \"Hey\", \"Good day\", \"How are you?\"],\n",
        "      \"responses\": [\"Hello!\", \"Good to see you!\", \"Hi there, how can I help?\"],\n",
        "      \"context_set\": \"\"\n",
        "    },\n",
        "    {\n",
        "      \"tag\": \"farewell\",\n",
        "      \"patterns\": [\"Goodbye\", \"Bye\", \"See you later\", \"Talk to you later\"],\n",
        "      \"responses\": [\"Sad to see you go :(\", \"Goodbye!\", \"Come back soon!\"],\n",
        "      \"context_set\": \"\"\n",
        "    },\n",
        "    {\n",
        "      \"tag\": \"creator\",\n",
        "      \"patterns\": [\"Who created you?\", \"Who is your developer?\", \"Who made you?\"],\n",
        "      \"responses\": [\"I was created by Lakshmi Kommuri.\"],\n",
        "      \"context_set\": \"\"\n",
        "    },\n",
        "    {\n",
        "      \"tag\": \"identity\",\n",
        "      \"patterns\": [\"What is your name?\", \"What should I call you?\", \"Who are you?\"],\n",
        "      \"responses\": [\"You can call me Mind Reader. I'm a Chatbot.\"],\n",
        "      \"context_set\": \"\"\n",
        "    },\n",
        "    {\n",
        "      \"tag\": \"hours\",\n",
        "      \"patterns\": [\"What are the college timings?\", \"When is the college open?\", \"What are your hours of operation?\"],\n",
        "      \"responses\": [\"The college is open from 8am to 5pm, Monday to Saturday.\"],\n",
        "      \"context_set\": \"\"\n",
        "    },\n",
        "    {\n",
        "      \"tag\": \"contact\",\n",
        "      \"patterns\": [\"How can I contact the college?\", \"What is the college telephone number?\", \"Can I get your contact number?\"],\n",
        "      \"responses\": [\"You can contact the college at 123456789.\"],\n",
        "      \"context_set\": \"\"\n",
        "    },\n",
        "    {\n",
        "      \"tag\": \"income\",\n",
        "      \"patterns\": [\"What is the average income of CUST1000?\", \"What is the average income of cust1000?\", \"avg income of CUST1000  ?\"],\n",
        "      \"responses\": [\"$59544\"],\n",
        "      \"context_set\": \"\"\n",
        "    },\n",
        "    {\n",
        "      \"tag\": \"avg income total\",\n",
        "      \"patterns\": [\"What is the average income of all customer?\", \"What is the average income?\", \"avg income of all?\"],\n",
        "      \"responses\": [\"51021.65469\"],\n",
        "      \"context_set\": \"\"\n",
        "    },\n",
        "    {\n",
        "      \"tag\": \"age\",\n",
        "      \"patterns\": [\"How many customers are under 30?\", \"customers below 30?\", \"Number of customers below 30?\"],\n",
        "      \"responses\": [\"141\"],\n",
        "      \"context_set\": \"\"\n",
        "    },\n",
        "    {\n",
        "      \"tag\": \"loan\",\n",
        "      \"patterns\": [\"Compare loan defaults by gender\", \"loan by gender\", \"comparision of loan by gender\"],\n",
        "      \"responses\": [\"https://drive.google.com/file/d/1_QqD6WUwXbfU_rYTOk7rKWZ3XuP-CazX/view?usp=drive_link\"],\n",
        "      \"context_set\": \"\"\n",
        "    },\n",
        "    {\n",
        "      \"tag\": \"count\",\n",
        "      \"patterns\": [\"Show a bar chart of transaction count by job\", \"bar chart of transaction count by job\", \"bar chat of transaction\"],\n",
        "      \"responses\": [\"https://drive.google.com/file/d/1G5WBTMlxJT_Ha5reFsmdSPPFSjhqpJqP/view?usp=sharing\"],\n",
        "      \"context_set\": \"\"\n",
        "    },\n",
        "  ]\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7hDjGRW1rjHZ"
      },
      "outputs": [],
      "source": [
        "# Function to perform synonym replacement\n",
        "def synonym_replacement(tokens, limit):\n",
        "    augmented_sentences = []\n",
        "    for i in range(len(tokens)):\n",
        "        synonyms = []\n",
        "        for syn in wordnet.synsets(tokens[i]):\n",
        "            for lemma in syn.lemmas():\n",
        "                synonyms.append(lemma.name())\n",
        "        if len(synonyms) > 0:\n",
        "            num_augmentations = min(limit, len(synonyms))\n",
        "            sampled_synonyms = random.sample(synonyms, num_augmentations)\n",
        "            for synonym in sampled_synonyms:\n",
        "                augmented_tokens = tokens[:i] + [synonym] + tokens[i+1:]\n",
        "                augmented_sentences.append(' '.join(augmented_tokens))\n",
        "    return augmented_sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KaJOgSLI2goX"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('wordnet')\n",
        "text_data = []\n",
        "labels = []\n",
        "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "\n",
        "limit_per_tag = 40\n",
        "\n",
        "for intent in intents['intents']:\n",
        "    augmented_sentences_per_tag = 0\n",
        "    for example in intent['patterns']:\n",
        "        tokens = nltk.word_tokenize(example.lower())\n",
        "        filtered_tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stopwords and token.isalpha()]\n",
        "        if filtered_tokens:\n",
        "            text_data.append(' '.join(filtered_tokens))\n",
        "            labels.append(intent['tag'])\n",
        "\n",
        "            augmented_sentences = synonym_replacement(filtered_tokens, limit_per_tag - augmented_sentences_per_tag)\n",
        "            for augmented_sentence in augmented_sentences:\n",
        "                text_data.append(augmented_sentence)\n",
        "                labels.append(intent['tag'])\n",
        "                augmented_sentences_per_tag += 1\n",
        "                if augmented_sentences_per_tag >= limit_per_tag:\n",
        "                    break\n",
        "\n",
        "print(len(text_data))\n",
        "print(len(labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bgXxm4ld2lM1"
      },
      "outputs": [],
      "source": [
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(text_data)\n",
        "y = labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GERrS3VZ2o4F"
      },
      "outputs": [],
      "source": [
        "def find_best_model(X, y, test_size=0.2):\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=100)\n",
        "\n",
        "\n",
        "    models = [\n",
        "        ('Logistic Regression', LogisticRegression(), {\n",
        "            'penalty': ['l2'],\n",
        "            'C': [0.1, 1.0, 10.0],\n",
        "            'solver': ['liblinear'],\n",
        "            'max_iter': [100, 1000, 10000]\n",
        "        }),\n",
        "        ('Multinomial Naive Bayes', MultinomialNB(), {'alpha': [0.1, 0.5, 1.0]}),\n",
        "        ('Linear SVC', LinearSVC(), {\n",
        "            'penalty': ['l2'],\n",
        "            'loss': ['hinge', 'squared_hinge'],\n",
        "            'C': [0.1, 1, 10],\n",
        "            'max_iter': [100, 1000, 10000]\n",
        "        }),\n",
        "        ('Decision Tree', DecisionTreeClassifier(), {\n",
        "            'max_depth': [5, 10, 20, None],\n",
        "            'min_samples_split': [2, 5, 10],\n",
        "            'min_samples_leaf': [1, 2, 4],\n",
        "            'criterion': ['gini', 'entropy']\n",
        "        }),\n",
        "        ('Random Forest', RandomForestClassifier(), {\n",
        "            'n_estimators': [100, 200, 300],\n",
        "            'max_depth': [10, 20, None],\n",
        "            'min_samples_split': [2, 5, 10],\n",
        "            'min_samples_leaf': [1, 2, 4]\n",
        "        })\n",
        "    ]\n",
        "\n",
        "    for name, model, param_grid in models:\n",
        "        grid = GridSearchCV(model, param_grid, cv=3, n_jobs=-1)\n",
        "        grid.fit(X_train, y_train)\n",
        "        y_pred = grid.predict(X_test)\n",
        "        score = accuracy_score(y_test, y_pred)\n",
        "        print(f'{name}: {score:.4f} (best parameters: {grid.best_params_})')\n",
        "\n",
        "    best_model = max(models, key=lambda x: GridSearchCV(x[1], x[2], cv=3, n_jobs=-1).fit(X_train, y_train).score(X_test, y_test))\n",
        "    print(f'\\nBest model: {best_model[0]}')\n",
        "\n",
        "    # Fit the best model to the full training data\n",
        "    best_model[1].fit(X, y)\n",
        "\n",
        "    return best_model[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j1oceEum2tA1"
      },
      "outputs": [],
      "source": [
        "best_model = find_best_model(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9lzkWWIt2vEc"
      },
      "outputs": [],
      "source": [
        "def chatbot_response(user_input):\n",
        "    input_text = vectorizer.transform([user_input])\n",
        "    predicted_intent = best_model.predict(input_text)[0]\n",
        "\n",
        "    for intent in intents['intents']:\n",
        "        if intent['tag'] == predicted_intent:\n",
        "            response = random.choice(intent['responses'])\n",
        "            break\n",
        "\n",
        "    return response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "zDqZf6xw3AFF",
        "outputId": "73d437c7-4521-435a-8361-7e3e15881fc1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hello! I am a chatbot. How can I help you today? Type \"quit\" to exit.\n",
            "Goodbye!\n",
            "$59544\n",
            "Goodbye!\n",
            "Sad to see you go :(\n",
            "Come back soon!\n"
          ]
        }
      ],
      "source": [
        "print('Hello! I am a chatbot. How can I help you today? Type \"quit\" to exit.')\n",
        "while True:\n",
        "    user_input = input('> ')\n",
        "    if user_input.lower() == 'quit':\n",
        "        break\n",
        "    response = chatbot_response(user_input)\n",
        "    print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7RALN64f3CYl"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pickle\n",
        "\n",
        "\n",
        "if not os.path.exists('model'):\n",
        "    os.makedirs('model')\n",
        "\n",
        "if not os.path.exists('dataset'):\n",
        "    os.makedirs('dataset')\n",
        "\n",
        "# Save the trained model\n",
        "with open('model/chatbot_model.pkl', 'wb') as f:\n",
        "    pickle.dump(best_model, f)\n",
        "\n",
        "# Save the vectorizer\n",
        "with open('model/vectorizer.pkl', 'wb') as f:\n",
        "    pickle.dump(vectorizer, f)\n",
        "\n",
        "# Save the intents to the \"dataset\" folder\n",
        "with open('dataset/intents1.json', 'w') as f:\n",
        "    json.dump(intents, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hX5vnEIQ9hMQ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}